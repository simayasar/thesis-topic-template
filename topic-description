\documentclass[a4paper,12pt]{article}

\usepackage{titlesec}
\titleformat*{\section}{\large\bfseries}
\titleformat*{\subsection}{\normalsize\bfseries}

\pagestyle{empty}

\usepackage[utf8]{inputenc}
\usepackage[top=0.8in, bottom=0.8in, left=0.8in, right=0.8in]{geometry}

\usepackage{graphicx}
\usepackage{url}
\usepackage{paralist}
\usepackage{lmodern}
\usepackage[authoryear]{natbib}
\renewcommand\textbullet{\ensuremath{\bullet}}
\renewcommand{\familydefault}{\sfdefault}

\date{}

\title{
\includegraphics[width=6cm]{figures/stuttgart-vector.pdf}\hfill{\includegraphics[width=3cm]{figures/sqa_logo.png}}
\\[0.7cm]
{\large \textit{Master's Thesis (Master Softwaretechnik):}}\\[2mm]
{\Large LLM-Based Transformation of Deployment Technologies into EDMM Models}
}

\begin{document}

\maketitle
\thispagestyle{empty}
\vspace{-2cm}

% ===============================
\subsection*{Background and Motivation}
Modern deployment technologies (e.g., Kubernetes, Terraform, Docker Compose, Ansible)
use different syntaxes and structures. This makes the transformation of deployment
models into a unified, technology-independent representation is challenging.  
The Essential Deployment Metamodel (EDMM) provides such an abstraction, but existing
rule-based transformation approaches like DeMAF require manually defined rules for each
technology, limiting scalability and maintainability.  
Large Language Models (LLMs), with strong capabilities in structured data understanding,
offer a promising alternative for performing these transformations more flexibly.

% ===============================
\subsection*{Goals}
This thesis aims to develop and evaluate an LLM-based method to transform deployment
technologies into EDMM models. The goals include:
\begin{itemize}
    \item Designing a RAG-based transformation pipeline using Ollama, Hugging Face embeddings, and ChromaDB.
    \item Automatically generating EDMM-compliant models from various deployment technologies.
    \item Comparing LLM-based outputs with rule-based approaches in terms of correctness, flexibility, and maintenance effort.
    \item Optionally, if time permits, conducting small-scale fine-tuning experiments (e.g., LoRA) to compare with the RAG-only approach.
\end{itemize}

% ===============================
\subsection*{Method}
The method is based on a Retrieval-Augmented Generation (RAG) workflow:
\begin{itemize}
    \item \textbf{Ollama} for local, privacy-preserving LLM inference.
    \item \textbf{Hugging Face embeddings} for semantic encoding of examples.
    \item \textbf{ChromaDB} as a vector store for retrieving relevant deployment snippets.
    \item \textbf{Prompt engineering} to enforce EDMM-compliant output formats.
\end{itemize}

If time allows, lightweight fine-tuning experiments (e.g., LoRA or API-based)
will be explored to measure whether fine-tuned models outperform a pure RAG approach.

% ===============================
\subsection*{Expected Results}
It is expected that LLM-based methods:
\begin{itemize}
    \item Achieve correctness comparable to or better than rule-based approaches.
    \item Show higher flexibility in handling new or unseen deployment technologies.
    \item Requires less manual maintenance compared to writing new transformation rules.
\end{itemize}
If fine-tuning can be performed, it may provide additional insights into where it
outperforms or complements a RAG approach.

% ===============================
\subsection*{Possible Collaborations}
Collaboration with the EDMM/DeMAF research group at the University of Stuttgart
may be possible to validate the transformation outputs and align EDMM structures.

% ===============================
\begin{scriptsize}
\bibliographystyle{abbrv}
\bibliography{bibliography.bib}
\end{scriptsize}

% ===============================
\subsection*{Contact}
Prof. Dr.-Ing. Steffen Becker, steffen.becker@iste.uni-stuttgart.de\\
University of Stuttgart, Institute of Software Engineering, SQA Group

\end{document}
